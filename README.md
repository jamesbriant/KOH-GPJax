# KOH-GPJax

## Introduction

KOH-GPJax is an extension to the [GPJax](https://github.com/JaxGaussianProcesses/GPJax) Python package which implements the [Kennedy & O'Hagan (2001)](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00294)[^1] Bayesian Calibration of Computer Models framework.

By combining the power of [Jax](https://jax.readthedocs.io/en/latest/) with the excellent modular design of GPJax KOH-GPJax aims to provide a Bayesian calibration framework for large-scale computer simulations.

This package is very much a work in progress. Please get in touch if you're interested in the package.

## Installation

Currently only available on GitHub.

```bash
pip install git+https://github.com/jamesbriant/KOH-GPJax.git
```

## Where to Start

If you are not familiar with using [GPJax](https://github.com/JaxGaussianProcesses/GPJax), start there. Once you are familiar with GPJax, come back to here.

First, we need to define the our `OurModel` by inheriting from `kohgpjax.base.AbstractKOHModel`.
This abstract class requires us to implement 5 methods. The first 4 are kernels and we use GPJax's framework to implement these.
The naming of these methods is consistent with Kennedy & O'Hagan's nomenclature. More specifically:

- `k_eta()` is the kernel for the underlying structure of the simulator output. Remember to include every input dimension, including the calibration parameters.
- `k_delta()` is the kernel of the discrepancy function.
- `k_epsilon()` is the kernel for the observation noise. This is often assumed to be IID, so a simple white noise kernel works well here.
- `k_epsilon_eta()` is the kernel for the structural error generated by the simulation. This is often VERY small in magnitude. Again, a white noise kernel works well.

We also need to define the `KOH_log_prior()` method. 

Remember to keep track of the parameters you are introducing, they all need a prior distribution!
In the example below, we have $p=1$ control (regression) parameter, $x$, and $q=1$ calibration parameter, $\theta$. Hence `k_eta` requires $p+q=2$ inputs whereas the other kernels require only $p=1$.

Suppose the example below is saved under `models/ourmodel.py`.

```python3
import gpjax as gpx
import jax.numpy as jnp
from jaxtyping import Float

from kohgpjax.base import AbstractKOHModel

class OurModel(AbstractKOHModel):
    def k_eta(self, GPJAX_params) -> gpx.kernels.AbstractKernel:
        thetas, ells, lambdas = GPJAX_params
        return gpx.kernels.ProductKernel(
            kernels=[
                gpx.kernels.RBF(
                    active_dims=[0],
                    lengthscale=jnp.array(ells[0]),
                    variance=jnp.array(1/lambdas[0])
                ), 
                gpx.kernels.RBF(
                    active_dims=[1],
                    lengthscale=jnp.array(ells[1]),
                )
            ]
        )
    
    def k_delta(self, GPJAX_params) -> gpx.kernels.AbstractKernel:
        thetas, ells, lambdas = GPJAX_params
        return gpx.kernels.RBF(
                active_dims=[0],
                lengthscale=jnp.array(ells[2]),
                variance=jnp.array(1/lambdas[1])
            )
    
    def k_epsilon(self, GPJAX_params) -> gpx.kernels.AbstractKernel:
        thetas, ells, lambdas = GPJAX_params
        return gpx.kernels.White(
                active_dims=[0],
                variance=jnp.array(1/lambdas[2])
            )
    
    def k_epsilon_eta(self, GPJAX_params) -> gpx.kernels.AbstractKernel:
        thetas, ells, lambdas = GPJAX_params
        return gpx.kernels.White(
                active_dims=[0],
                variance=jnp.array(1/lambdas[3])
            )

    def KOH_log_prior(
        self,
        GPJAX_params,
    ) -> Float:
        thetas, ells, lambdas = GPJAX_params

        ####### ell #######
        # lengthscale parameters
        # EXAMPLE: ell_eta_0 ~ GAM(4,1.4) where 2nd param is rate
        logprior = (4-1)*jnp.log(ells[0]) - 1.4*ells[0]

        # Prior for ell_eta_1 ~ GAM(2,3.5) where 2nd param is rate
        logprior += (2-1)*jnp.log(ells[1]) - 3.5*ells[1]

        # Prior for ell_delta_0 ~ GAM(4,2) where 2nd param is rate
        logprior += (4-1)*jnp.log(ells[2]) - 2*ells[2]


        ####### lambda #######
        # variance parameters
        # EXAMPLE: lambda_eta ~ GAM(2,4) where 2nd param is rate
        logprior += (2-1)*jnp.log(lambdas[0]) - 4*lambdas[0]

        # EXAMPLE: lambda_b ~ GAM(2,0.1) where 2nd param is rate
        logprior += (2-1)*jnp.log(lambdas[1]) - 0.1*lambdas[1]

        # EXAMPLE: lambda_e ~ GAM(12,0.025) where 2nd param is rate
        logprior += (12-1)*jnp.log(lambdas[2]) - 0.025*lambdas[2]

        # EXAMPLE: lambda_en ~ GAM(10,0.001) where 2nd param is rate
        logprior += (10-1)*jnp.log(lambdas[3]) - 0.001*lambdas[3]

        return logprior
```

We also need to develop a function which will transform the parameters from our optimiser/MCMC/VI scheme to the constrained domain expected by GPJax. For example, many MCMC methods assume parameter operate on the real line, but we require both our lengthscale and variance parameters of our kernels to be strictly positive.

```python3
param_transform_mici_to_gpjax = lambda x: [
    [ # theta (calibration) parameters
        mapRto01(x[0]),
    ],
    [ # lengthscale parameters
        mapRto0inf(x[1]), 
        mapRto0inf(x[2]), 
        mapRto0inf(x[3]),
    ],
    [ # lambda (variance) parameters
        mapRto0inf(x[4]), 
        mapRto0inf(x[5]), 
        mapRto0inf(x[6]), 
        mapRto0inf(x[7]),
    ]
]
```

Finally, we import `OurModel()`, provide the data and retrieve the negative log posterior density function. This function can be provided directly to your optimiser/MCMC/VI method.

```python3
from jax import config
config.update("jax_enable_x64", True)

import gpjax as gpx
from jax import jit
import jax.numpy as jnp
import kohgpjax as kgx

from models.ourmodel import OurModel

# Import the data however you see fit.
dataloader = DataLoader('data/toy/field.csv', 'data/toy/sim.csv')
data = dataloader.get_data()
model = KOHmodel.Model(*data)

nlpd_func = model.get_KOH_neg_log_pos_dens_func(
    param_transform_mici_to_gpjax
)

# You can jax.jit and jax.grad this function!
nlpd_jit = jit(nlpd_func)

x_test = jnp.array([
    0.5,        # theta
    1.0,        # ell_eta_0
    0.3,        # ell_eta_1
    1.0,        # ell_delta_0
    1.0,        # lambda_eta
    30.0,       # lambda_delta
    400.0,      # lambda_epsilon
    10000.0,    # lambda_epsilon_eta
])
nlpd_jit(x_test)
```

Visit my [BayesianCalibrationExamples](https://github.com/jamesbriant/BayesianCalibrationExamples) repository for many examples using this package.

## To-Do

Very vague outline of this project:

- [x] Create a modular KOH kernel class
- [x] Build a posterior class
- [x] Add KOHDataset class
- [ ] Improve the data flow within the AbstractKOHModel class.
- [ ] Standardise `GPJAX_params`.

## References

[^1]: Kennedy, M.C. and O'Hagan, A. (2001), Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63: 425-464. [https://doi.org/10.1111/1467-9868.00294](https://doi.org/10.1111/1467-9868.00294)
