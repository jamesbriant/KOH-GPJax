# KOH-GPJax

## Introduction

KOH-GPJax is an extension to the [GPJax](https://github.com/JaxGaussianProcesses/GPJax) Python package which implements the [Kennedy & O'Hagan (2001)](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00294)[^1] Bayesian Calibration of Computer Models framework.

By combining the power of [Jax](https://jax.readthedocs.io/en/latest/) with the excellent modular design of GPJax KOH-GPJax aims to provide a Bayesian calibration framework for large-scale computer simulations.

This package is very much a work in progress. Please get in touch if you're interested in the package.

## Installation

Currently only available on GitHub.

```bash
pip install git+https://github.com/jamesbriant/KOH-GPJax.git
```

## Where to Start

If you are not familiar with using [GPJax](https://github.com/JaxGaussianProcesses/GPJax), start there. Once you are familiar with GPJax, come back to here.

First, we need to define the our `OurModel` by inheriting from `kohgpjax.base.AbstractKOHModel`.
This abstract class requires us to implement 5 methods. The first 4 are kernels and we use GPJax's framework to implement these.
The naming of these methods is consistent with Kennedy & O'Hagan's nomenclature. More specifically:

- `k_eta()` is the kernel for the underlying structure of the simulator output. Remember to include every input dimension, including the calibration parameters.
- `k_delta()` is the kernel of the discrepancy function.
- `k_epsilon()` is the kernel for the observation noise. This is often assumed to be IID, so a simple white noise kernel works well here.
- `k_epsilon_eta()` is the kernel for the structural error generated by the simulation. This is often VERY small in magnitude. Again, a white noise kernel works well.

We also need to define the `KOH_log_prior()` method. 

Remember to keep track of the parameters you are introducing, they all need a prior distribution!
In the example below, we have $p=1$ control (regression) parameter, $x$, and $q=1$ calibration parameter, $\theta$. Hence `k_eta` requires $p+q=2$ inputs whereas the other kernels require only $p=1$.

Suppose the example below is saved under `models/ourmodel.py`.

```python3
import gpjax as gpx
import jax.numpy as jnp
from jaxtyping import Float

from kohgpjax.kohmodel import KOHModel

class Model(KOHModel):
    def k_eta(self, eta_params_constrained) -> gpx.kernels.AbstractKernel:
        return gpx.kernels.ProductKernel(
            kernels=[
                gpx.kernels.RBF(
                    active_dims=[0],
                    lengthscale=jnp.array(eta_params_constrained['lengthscale']['x_0']),
                    variance=jnp.array(1/eta_params_constrained['variance']),
                ), 
                gpx.kernels.RBF(
                    active_dims=[1],
                    lengthscale=jnp.array(eta_params_constrained['lengthscale']['theta_0']),
                )
            ]
        )
    
    def k_delta(self, delta_params_constrained) -> gpx.kernels.AbstractKernel:
        return gpx.kernels.RBF(
            active_dims=[0],
            lengthscale=jnp.array(delta_params_constrained['lengthscale']['x_0']),
            variance=jnp.array(1/delta_params_constrained['variance']),
        )
    
    def k_epsilon(self, epsilon_params_constrained) -> gpx.kernels.AbstractKernel:
        return gpx.kernels.White(
            active_dims=[0],
            variance=jnp.array(1/epsilon_params_constrained['variance']),
        )
    
    def k_epsilon_eta(self, epsilon_eta_params_constrained) -> gpx.kernels.AbstractKernel:
        return gpx.kernels.White(
            active_dims=[0],
            variance=jnp.array(1/epsilon_eta_params_constrained['variance'])
        )
```

Next we need to define the prior distributions and bijectors for each model parameter. This is accomplished using a dictionary as follows.

```python3
import distrax
import jax.numpy as jnp

from kohgpjax.parameters import (
    ParameterPrior,
    PriorDict,
)

def map01toR(theta) -> Float:
    return jnp.log(-1+1/theta)

def map0inftoR(theta) -> Float:
    return jnp.log(theta)

map01toR_distrax = distrax.Lambda(
    lambda x: map01toR(x)
)
map0inftoR_distrax = distrax.Lambda(
    lambda x: map0inftoR(x)
)

prior_dict: PriorDict = {
    'thetas': {
        'theta_0': ParameterPrior(
            distrax.Uniform(low=0.0, high=1.0),
            map01toR_distrax,
        ),
    },
    'eta': {
        'variance': ParameterPrior(
            distrax.Gamma(concentration=2.0, rate=4.0), 
            map0inftoR_distrax,
        ),
        'lengthscale': {
            'x_0': ParameterPrior(
                distrax.Gamma(concentration=4.0, rate=1.4),
                map0inftoR_distrax,
            ),
            'theta_0': ParameterPrior(
                distrax.Gamma(concentration=2.0, rate=3.5),
                map0inftoR_distrax,
            ),
        },
    },
    'delta': {
        'variance': ParameterPrior(
            distrax.Gamma(concentration=2.0, rate=0.1),
            map0inftoR_distrax,
        ),
        'lengthscale': {
            'x_0': ParameterPrior(
                distrax.Gamma(concentration=4.0, rate=2.0),
                map0inftoR_distrax,
            ),
        },
    },
    'epsilon': {
        'variance': ParameterPrior(
            distrax.Gamma(concentration=12.0, rate=0.025),
            map0inftoR_distrax,
        ),
    },
    'epsilon_eta': { #TODO: make this optional
        'variance': ParameterPrior(
            distrax.Gamma(concentration=10.0, rate=0.001),
            map0inftoR_distrax,
        ),
    },
}
```

Finally, we import `OurModel()`, provide the data and retrieve the negative log posterior density function. This function can be provided directly to your MCMC method.

```python3
from jax import config
config.update("jax_enable_x64", True)

import gpjax as gpx
from jax import jit
import jax.numpy as jnp
import kohgpjax as kgx

from models.ourmodel import OurModel

###### LOAD DATA ######
DATAFIELD = np.loadtxt('data/field.csv', delimiter=',', dtype=np.float32)
DATACOMP = np.loadtxt('data/comp.csv', delimiter=',', dtype=np.float32)

xf = jnp.reshape(DATAFIELD[:, 0], (-1, 1)).astype(jnp.float64)
xc = jnp.reshape(DATACOMP[:, 0], (-1,1)).astype(jnp.float64)
tc = jnp.reshape(DATACOMP[:, 1], (-1,1)).astype(jnp.float64)
yf = jnp.reshape(DATAFIELD[:, 1], (-1,1)).astype(jnp.float64)
yc = jnp.reshape(DATACOMP[:, 2], (-1,1)).astype(jnp.float64)

tmin = jnp.min(tc)
tmax = jnp.max(tc)
tc_normalized = (tc - tmin)/(tmax - tmin)

field_dataset = gpx.Dataset(xf, yf)
comp_dataset = gpx.Dataset(jnp.hstack((xc, tc_normalized)), yc)

kohdataset = kgx.KOHDataset(field_dataset, comp_dataset)


###### Create Model Instance and JIT ######
model = Model(
    model_parameters=kgx.ModelParameters(prior_dict=prior_dict),
    kohdataset=kohdataset,
)

nlpd_jit = jax.jit(
    model.get_KOH_neg_log_pos_dens_func()
)

###### TEST ######
prior_leaves, prior_tree = jax.tree.flatten(prior_dict)
prior_means = jax.tree.map(
    lambda x: x.forward(x.distribution.mean()), prior_leaves
)
x_test = jnp.array(prior_means)

nlpd_jit(x_test)
```

Visit my [BayesianCalibrationExamples](https://github.com/jamesbriant/BayesianCalibrationExamples/) repository for many examples using this package.

## To-Do

Very vague outline of this project:

- [x] Create a modular KOH kernel class
- [x] Build a posterior class
- [x] Add KOHDataset class
- [x] Improve the data flow within the AbstractKOHModel class.
- [x] Standardise `GPJAX_params`.

## References

[^1]: Kennedy, M.C. and O'Hagan, A. (2001), Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63: 425-464. [https://doi.org/10.1111/1467-9868.00294](https://doi.org/10.1111/1467-9868.00294)
